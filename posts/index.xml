<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Njord tech blog</title><link>www.njordy.com/posts/</link><description>Recent content in Posts on Njord tech blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 23 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="www.njordy.com/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Storing Kubeflow Pipeline Templates in GCP Artifact Registry</title><link>www.njordy.com/2023/03/23/storing-kubeflow-pipeline-templates-gcp-artifact-registry/</link><pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/03/23/storing-kubeflow-pipeline-templates-gcp-artifact-registry/</guid><description>In this blog post, we will discuss how to store Kubeflow Pipeline templates in GCP Artifact Registry, enabling reusability and version control for your pipelines. Using Artifact Registry over Cloud Storage simplifies version control and allows for easier collaboration between single or multiple users.
The Kubeflow Pipelines SDK registry client is a new client interface that you can use with a compatible registry server (ensure you are using the correct KFP version), such as Artifact Registry, for version control of your Kubeflow Pipelines (KFP) templates.</description></item><item><title>Running a kubeflow pipeline on google vertex</title><link>www.njordy.com/2023/03/21/kubeflow-pipelines/</link><pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/03/21/kubeflow-pipelines/</guid><description>This blog post will go over how to build and run your very first kubeflow pipeline (kfp). In short, Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.
There are a lot of possibilities to run the pipelines, but in this series, we will use gcp vertex pipelines. Vertex will be the runner, but the pipelines will follow the kubeflow conventions meaning you can run them on whatever platform at hand or host kubeflow on your own Kubernetes cluster.</description></item><item><title>Configuring Your Local Dev Container with GCP Default Credentials</title><link>www.njordy.com/2023/03/20/vs-code-dev-container-gcp-credentials/</link><pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/03/20/vs-code-dev-container-gcp-credentials/</guid><description>This blog post provides a step-by-step guide for setting up your VS Code dev container to work with Google Cloud Platform (GCP) services and APIs by configuring default GCP credentials. By authenticating your application with your GCP credentials, you can access the necessary resources without requiring additional authentication steps, saving time and streamlining your development workflow.
Configuring Default GCP Credentials To use default credentials with GCP, you can follow the steps below:</description></item><item><title>Supercharge Your Development Workflow with VS Code Dev Containers</title><link>www.njordy.com/2023/03/18/dev-containers/</link><pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/03/18/dev-containers/</guid><description>VS docker Dev Container Are you tired of dealing with messy, inconsistent development environments that slow down your workflow? Look no further than VS Code dev containers, a powerful feature in Visual Studio Code that can streamline your development process and improve consistency across different environments.
In this post, we&amp;rsquo;ll dive into what dev containers are and how you can use them to supercharge your development workflow. We&amp;rsquo;ll also walk through how to set up a dev container for a Python environment and you will get an understsnding how easy it is to get started.</description></item><item><title>Postgress extention using ChatGPT3</title><link>www.njordy.com/2023/03/11/postgress_extention_using_chatgpt3/</link><pubDate>Sat, 11 Mar 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/03/11/postgress_extention_using_chatgpt3/</guid><description>This blog post will dive into building postgres extensions using pgx in rust. In order to do something existing we will ride the hype curve and integrate ChatGPT3 into postgres.
TLDR: repo
Setup First step is to set up pgx:
$ cargo install --locked cargo-pgx $ cargo pgx init We will then create a new create using:
$ cargo pgx new my_extension $ cd my_extension Which should give you something like:</description></item><item><title>Go memory arena</title><link>www.njordy.com/2023/03/01/go_memory_arena/</link><pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/03/01/go_memory_arena/</guid><description>As part of the go 1.20 release memory areas where introduced to the standard lib but not mentioned in the release notes but is still being discussed as of 2023-03-02 here. Memory arenas allow users to allocate memory and are described by the docs as:
The arena package provides the ability to allocate memory for a collection of Go values and free that space manually all at once, safely. The purpose of this functionality is to improve efficiency: manually freeing memory before a garbage collection delays that cycle.</description></item><item><title>Triton shared memory and pinned memory</title><link>www.njordy.com/2023/02/25/triton_shared_memory/</link><pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/02/25/triton_shared_memory/</guid><description>This blog post will go in to depth how to use shared memory together with nvidia triton and pinned memory for model serving. This will continue to build further on the other blog posts related to triton. First we will focuse on shared memory and then move over to also look in to pinned memory and why it matters.
Shared memory In the triton examples(python) shared memory is often abbreviated as shm.</description></item><item><title>Duckdb with hugo</title><link>www.njordy.com/2023/02/06/hugo-duckdb/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/02/06/hugo-duckdb/</guid><description>Current hack to just get the stuff working.</description></item><item><title>Go compiler optimizations</title><link>www.njordy.com/2023/02/08/Profile-guided_inlining_optimization/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/02/08/Profile-guided_inlining_optimization/</guid><description>This is based upon the new feature released in go v1.20 where the compiler can optimize using a pprof file.
In order to run the pprof we will use flags:
flag.Parse() if *cpuprofile != &amp;#34;&amp;#34; { f, err := os.Create(*cpuprofile) if err != nil { log.Fatal(err) } pprof.StartCPUProfile(f) defer pprof.StopCPUProfile() } more info can be found here.
In order to run the profiling use the following command:
go run main.go -cpuprofile=prof.</description></item><item><title>Shinylive app with hugo</title><link>www.njordy.com/2023/02/06/hugo-with-python-rshiny/</link><pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/02/06/hugo-with-python-rshiny/</guid><description>Shinylive app with hugo This blog post is based upon RamiKrispin/shinylive where we will take a look in to using Shiny with python and leveraging WebAssembly to let it run in the browser with out a backend. This allows for interactive static webpages.
In order to add the a shiny app it needs to be deployed, in this case that is handled through github pages and lives within https://github.com/NikeNano/shinylive. The second step is to add the iframe:</description></item><item><title>Trition with post and pre processing</title><link>www.njordy.com/2023/02/01/Trition_with_post_and_pre_processing/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/02/01/Trition_with_post_and_pre_processing/</guid><description>Trition with post and pre processing. This is based upon this repo
In this blog post we will dig down in to how a Machine Learning(ML) model can be combined with pre and post processing steps using Nvidia triton. By combining the pre- and post processing the user can make a single call using GPRC or http. It should be noted that we in reality will not merge these processing steps in any way but link the calls together using Tritons ensemble functionality Triton support multiple different backends(processing functionality) and in this case we will use the tensorRT backend for the model serving and the python backend to add the pre and post processing business logic.</description></item><item><title>Trition with post and pre processing</title><link>www.njordy.com/2023/02/01/Trition_with_post_and_pre_processing/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>www.njordy.com/2023/02/01/Trition_with_post_and_pre_processing/</guid><description>Prompt engineering With the rise of GPT-3 and Stable diffusion the concpet of promt engineering has gain more and more traction. According to wikipedia the task can be described as
Prompt engineering typically works by converting one or more tasks to a prompt-based dataset and training a language model with what has been called &amp;ldquo;prompt-based learning&amp;rdquo; or just &amp;ldquo;prompt learning&amp;rdquo;
Wikipedia Prompts are inputs for models that expect text as input however the output can very most famously images or text.</description></item><item><title>Welcome to the Blog</title><link>www.njordy.com/2022/07/17/hello-world/</link><pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate><guid>www.njordy.com/2022/07/17/hello-world/</guid><description> “Yeah It&amp;rsquo;s on. ”
Hello World!</description></item><item><title/><link>www.njordy.com/posts/2023-02-07-poor_mans_datalake/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>www.njordy.com/posts/2023-02-07-poor_mans_datalake/</guid><description>Poor mans datalake layout: post title: &amp;ldquo;Poor mans datalake&amp;rdquo; subtitle: &amp;ldquo;DuckDb&amp;rdquo; date: 2023-02-01 author: &amp;ldquo;Niklas Hansson&amp;rdquo; URL: &amp;ldquo;/2023/02/01/Trition_with_post_and_pre_processing/&amp;rdquo; This post is a deep dive playing with DuckDB doing a twist on Build a poor man’s data lake from scratch with DuckDB where we will do the following changes:
Use Minio instead of S3 DBT instead of dagster. We will host it on Kubernetes and set it up so it all run locally.</description></item></channel></rss>