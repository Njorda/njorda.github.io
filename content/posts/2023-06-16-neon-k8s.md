---
layout: post
title: "How to run Serverless Postgres on K8s "
subtitle: "Serverless Postgres"
date: 2023-06-12
author: "Niklas Hansson"
URL: "/2023/06/12/serverless-postgres"
---


First part of the series of building a cloud vendor on your own. We will aim for a small amount of services and step by step see how we can get it up and running.

[Serverless postgres](https://github.com/neondatabase/neon)


# Docker compose to K8s

Will try out [Kompose](https://kompose.io/) which is a tool for converting docker compose files to k8s resources. 

Install
```bash
curl -L https://github.com/kubernetes/kompose/releases/download/v1.26.0/kompose-darwin-amd64 -o kompose
```

Clone the [neon repo](https://github.com/neondatabase/neon#running-local-installation) and move to the docker-compose folder. 

STEP 1 is to remove the specific docker compose network mapping that is not working as it should for the docker stuuuffff. 



```bash
kompose --file docker-compose.yml convert
```

set up minikube in a good state
``` bash
minikube stop
minikube config set memory 8192
minikube config set cpus 4
minikube start
```


```bash
minikube start
```
The convertion between docker-compse and k8s is good but not perfect so we will step by step deploy the resources and see what we need to make it work. Edit the compute resource to have the following image instead: 

```yaml
        - command: ["/usr/local/bin/compute_ctl", "--pgdata", "/var/db/postgres/compute", "-C", "postgresql://cloud_admin@localhost:55433/postgres", "-b","/usr/local/bin/postgres", "--spec-path", "/spec.json"]
          env:
            - name: PG_VERSION
              value: "15"
          image: neondatabase/compute-node-v15
```

We build a new container on top with the spec of interest. 

```bash
docker build -f Dockerfile -t compute2 .
docker tag compute2 nikenano/neon_compute:2306182134
docker push nikenano/neon_compute:2306182134
```

add port to the pageservice deployment ... not sure why not needed in the docker compose ... 

test locally using:
```bash
> docker run -it --entrypoint bash nikenano/neon_compute:2306182114
```

and remove the start up command! We will have to change this later on some how .... . Not sure how though. 

```bash
kubectl apply -f docker-compose
```

Now lets connect to the pageserver to create a tenant ... 

we first forward the page server pod in order to be able to access it(we could also forward the service if we like to)

```bash
kubectl port-forward pageserver-59544f599d-ckpjs 9898:9898
```

```bash
curl -X POST -H "Content-Type: application/json" -d '{"new_tenant_id": "de200bd42b49cc1814412c7e592dd6e9"}' http://localhost:9898/v1/tenant/
```

<!-- Wait with this one for now ... -->

```bash
curl  -X POST -H "Content-Type: application/json" -d '{"new_timeline_id": "de200bd42b49cc1814412c7e592dd6e7", "pg_version": 15}' http://localhost:9898/v1/tenant/de200bd42b49cc1814412c7e592dd6e9/timeline/
```

Lets update the compute node stuff:

```json

{
    "format_version": 1.0,

    "timestamp": "2022-10-12T18:00:00.000Z",
    "operation_uuid": "0f657b36-4b0f-4a2d-9c2e-1dcd615e7d8c",

    "cluster": {
        "cluster_id": "docker_compose",
        "name": "docker_compose_test",
        "state": "restarted",
        "roles": [
            {
                "name": "cloud_admin",
                "encrypted_password": "b093c0d3b281ba6da1eacc608620abd8",
                "options": null
            }
        ],
        "databases": [
        ],
        "settings": [
            {
                "name": "fsync",
                "value": "off",
                "vartype": "bool"
            },
            {
                "name": "wal_level",
                "value": "replica",
                "vartype": "enum"
            },
            {
                "name": "wal_log_hints",
                "value": "on",
                "vartype": "bool"
            },
            {
                "name": "log_connections",
                "value": "on",
                "vartype": "bool"
            },
            {
                "name": "port",
                "value": "55433",
                "vartype": "integer"
            },
            {
                "name": "shared_buffers",
                "value": "1MB",
                "vartype": "string"
            },
            {
                "name": "max_connections",
                "value": "100",
                "vartype": "integer"
            },
            {
                "name": "listen_addresses",
                "value": "0.0.0.0",
                "vartype": "string"
            },
            {
                "name": "max_wal_senders",
                "value": "10",
                "vartype": "integer"
            },
            {
                "name": "max_replication_slots",
                "value": "10",
                "vartype": "integer"
            },
            {
                "name": "wal_sender_timeout",
                "value": "5s",
                "vartype": "string"
            },
            {
                "name": "wal_keep_size",
                "value": "0",
                "vartype": "integer"
            },
            {
                "name": "password_encryption",
                "value": "md5",
                "vartype": "enum"
            },
            {
                "name": "restart_after_crash",
                "value": "off",
                "vartype": "bool"
            },
            {
                "name": "synchronous_standby_names",
                "value": "walproposer",
                "vartype": "string"
            },
            {
                "name": "shared_preload_libraries",
                "value": "neon",
                "vartype": "string"
            },
            {
                "name": "neon.safekeepers",
                "value": "safekeeper1:5454,safekeeper2:5454,safekeeper3:5454",
                "vartype": "string"
            },
            {
                "name": "neon.timeline_id",
                "value": "de200bd42b49cc1814412c7e592dd6e7",
                "vartype": "string"
            },
            {
                "name": "neon.tenant_id",
                "value": "de200bd42b49cc1814412c7e592dd6e9",
                "vartype": "string"
            },
            {
                "name": "neon.pageserver_connstring",
                "value": "host=pageserver port=6400",
                "vartype": "string"
            },
            {
                "name": "max_replication_write_lag",
                "value": "500MB",
                "vartype": "string"
            },
            {
                "name": "max_replication_flush_lag",
                "value": "10GB",
                "vartype": "string"
            }
        ]
    },

    "delta_operations": [
    ]
}

```


cant not ping  service , but you can curl it .... This is the way!!!

- 1 compute node per database as I understand ... 
- 1 tenant per database?

Continue here 


ERROR is : 

```bash
2023-06-18T20:29:41.681422Z ERROR could not start the compute node: failed to get basebackup@0/0 from pageserver host=pageserver port=6400

Caused by:
    0: db error: ERROR: Tenant de200bd42b49cc1814412c7e592dd6e9 not found
    1: ERROR: Tenant de200bd42b49cc1814412c7e592dd6e9 not found
```

I update the service to export the port for the page server now as well! I also update the container to expose the port! 

```bash
Caused by:
    0: db error: ERROR: Timeline de200bd42b49cc1814412c7e592dd6e9/de200bd42b49cc1814412c7e592dd6e7 was not found
    1: ERROR: Timeline de200bd42b49cc1814412c7e592dd6e9/de200bd42b49cc1814412c7e592dd6e7 was not found
```

error in the page service ....

```bash
Caused by:
    0: service error
    1: unhandled error
    2: unhandled error
    3: Error { code: "NoSuchBucket", message: "The specified bucket does not exist", aws_request_id: "1769DBABF5D6DAAB" }
2023-06-18T20:44:27.038380Z  INFO remote_upload{tenant=de200bd42b49cc1814412c7e592dd6e9 timeline=de200bd42b49cc1814412c7e592dd6e7 upload_task_id=1}: Backoff: waiting 3 seconds before processing with the task
2023-06-18T20:44:27.328948Z  INFO wal_connection_manager{tenant_id=de200bd42b49cc1814412c7e592dd6e9 timeline_id=de200bd42b49cc1814412c7e592dd6e7}: Attempt #66, failed to subscribe for timeline de200bd42b49cc1814412c7e592dd6e9/de200bd42b49cc1814412c7e592dd6e7 updates in broker: status: Unavailable, message: "error trying to connect: dns error: failed to lookup address information: Name or service not known", details: [], metadata: MetadataMap { headers: {} }
2023-06-18T20:44:27.329002Z  INFO wal_connection_manager{tenant_id=de200bd42b49cc1814412c7e592dd6e9 timeline_id=de200bd42b49cc1814412c7e592dd6e7}: Backoff: waiting 3 seconds before processing with the task
```

The error above is due to that the bucket(minio) is not created and this needs to be done! I will check out the docker-compose way of doing this: 

```yaml
    command:
      - "until (/usr/bin/mc alias set minio http://minio:9000 $$MINIO_ROOT_USER $$MINIO_ROOT_PASSWORD) do
             echo 'Waiting to start minio...' && sleep 1;
         done;
         /usr/bin/mc mb minio/neon --region=eu-north-1;
         exit 0;"
```

```bash
mc mb minio/neon --region=eu-north-1;
```

[docs](https://min.io/docs/minio/linux/reference/minio-mc/mc-mb.html) creates a new bucket. I created it through the UI, createing a bucket named `neon`, after runing 
```bash
curl  -X POST -H "Content-Type: application/json" -d '{"new_timeline_id": "de200bd42b49cc1814412c7e592dd6e7", "pg_version": 15}' http://localhost:9898/v1/tenant/de200bd42b49cc1814412c7e592dd6e9/timeline/
```

there is a new sub bucket called `pageserver`.


Access the compute node now: 


```bash
kubectl port-forward compute-5899c985b7-cgrd8 55433:55433
```

get some compute going ...

```bash
psql -h localhost -p 55433 -U cloud_admin postgres
```


Fix the generate storage broker deployment that has a unvalid name? 

```bash
The Deployment "storage-broker" is invalid: spec.template.spec.containers[0].name: Invalid value: "storage_broker": a lowercase RFC 1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?')
```

have to change the name to not use underscore `_` same for all referenaces of the service name ... missed this first!


Update the http url based upon this: https://stackoverflow.com/questions/34557758/unable-to-use-curl-and-service-name-from-inside-kubernetes-pod

CONTINUE WORKING ON IT, will solved it in the end!


This is an issue inside the spec document that needs to be fixed as well otherwise it will not work ... 

```json
            {
                "name": "neon.safekeepers",
                "value": "127.0.0.1:6502,127.0.0.1:6503,127.0.0.1:6501",
                "vartype": "string"
            },
```

```json
            {
                "name": "neon.safekeepers",
                "value": "127.0.0.1:6502,127.0.0.1:6503,127.0.0.1:6501",
                "vartype": "string"
            },
```


Lets try to do it in the exact order as the docker compose does it .... 


The current issue is that the page server dont have the correct connection to the safekeepers. This is controlled through the configuration that is set in page keepers. The default will not work ... this is kind of a hidden detail since docker compose will use local network? I think this is the current issue which is causing problems ... `control_plane/safekeepers.conf` is an example. This is the api: `/v1/tenant/config`


it seems like it takes it from the caller ..., the compute node needs to call the page server for it to be correct ... am I sane or is this crazy ... 

```rust
async fn build_timeline_info_common(
    timeline: &Arc<Timeline>,
    ctx: &RequestContext,
) -> anyhow::Result<TimelineInfo> {
```

```rust
    ///
    /// This way we ensure to keep up with the most up-to-date safekeeper and don't try to jump from one safekeeper to another too frequently.
    /// Both thresholds are configured per tenant.
    fn next_connection_candidate(&mut self) -> Option<NewWalConnectionCandidate> {
        self.cleanup_old_candidates();
```

this seems to be the issue here: 

```rust 
    fn register_timeline_update(&mut self, timeline_update: SafekeeperTimelineInfo) {
        WALRECEIVER_BROKER_UPDATES.inc();

        let new_safekeeper_id = NodeId(timeline_update.safekeeper_id);
        let old_entry = self.wal_stream_candidates.insert(
            new_safekeeper_id,
            BrokerSkTimeline {
                timeline: timeline_update,
                latest_update: Utc::now().naive_utc(),
            },
        );

        if old_entry.is_none() {
            info!("New SK node was added: {new_safekeeper_id}");
            WALRECEIVER_CANDIDATES_ADDED.inc();
        }
    }

```

here now ... 


```rust
    let conf = match initialize_config(&cfg_file_path, arg_matches, &workdir)? {
        ControlFlow::Continue(conf) => conf,
        ControlFlow::Break(()) => {
            info!("Pageserver config init successful");
            return Ok(());
        }
    };
```


Continue in the tmp repo and build it with logging. 
