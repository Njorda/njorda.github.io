<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Njord tech blog</title><link>https://www.njordy.com/</link><description>Recent content on Njord tech blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 19 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.njordy.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Learnings from writing recursive SQL</title><link>https://www.njordy.com/2024/01/19/recursive-sql-learnings/</link><pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2024/01/19/recursive-sql-learnings/</guid><description>In this blog post we will go over some learnings from writing recursive queries. For example data we will use the Recursive queries blog post and the set up provided in the post as well, and thus not go over how to bring up the env in this post.
Avoiding cycles Writing Recursive queries it is easy to have cycles that. For a process at work I tried to do a graph traversal but ended up in a infinite loop, to avoid spilling sensitive information we will rely on a mock example based upon this example.</description></item><item><title>SQL for gcloud bucket</title><link>https://www.njordy.com/2024/01/16/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2024/01/16/</guid><description>TLDR: https://github.com/Njorda/cloudsql/tree/main
The goal with this blog post is to build a small tool to query Google Cloud buckets. We will do this using ChatGPT, I feel like I need to be even better at prompting for coding help. So to start by setting some constraints to limit the scope and make it a reasonable task to finish in a hour we will use go and we will limit our self to SELECT with WHERE we will support predicate push downs but thats it.</description></item><item><title>Recursive queries</title><link>https://www.njordy.com/2024/01/10/recursive-sql/</link><pubDate>Wed, 10 Jan 2024 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2024/01/10/recursive-sql/</guid><description>I coupe of weeks ago I came across that Postgres supports recursive queries and thus we will take a deep dive in this blog post on the concept of recursion in SQL. The docs for postgres and recursive queries can be found here if you are not familiar with CTE I recommend you to start with that. In this blog post we will start with exploring this concept in Postgres before we jump to DuckDB.</description></item><item><title>How to set up Neon, serverless postgres on k8s</title><link>https://www.njordy.com/2023/11/12/serverless-postgres/</link><pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/11/12/serverless-postgres/</guid><description>All the code can be found here
In this blog post we will dive in to how to set up neon on your k8s cluster. We will use Minikube but feel free to use the setup of your choice of k8s. The first step is to define the k8s resources. In this case we will take a short cut and start of with generating them from the docker compose files used for testing.</description></item><item><title>How to finetune gpt3.5 turbo</title><link>https://www.njordy.com/2023/09/02/finetune-gpt3-5-turbo/</link><pubDate>Sat, 02 Sep 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/09/02/finetune-gpt3-5-turbo/</guid><description>Way finetune Finetuning gpt turbo seams to lead to a couple of improvements
Improved steerability Reliable output formatting Custom tone Decreased prompt size The code for this tutorial can be found here
Step by step guide First you need to create an openAI api_key. The eaiest way to do this is through the openAI web page under API keys. . Copy the key and add it to an env file with the name open_ai.</description></item><item><title>How to Quickly Run a Docker Image on GCP</title><link>https://www.njordy.com/2023/09/02/docker-iterations/</link><pubDate>Sat, 02 Sep 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/09/02/docker-iterations/</guid><description>Run docker container in GCP cloud shell In this blog post, we will briefly go over how to build and run a Docker container as quickly as possible on Google Cloud Platform (GCP).
First, you need to create a Docker image. Something very simple should suffice. Here is a sample Dockerfile:
FROM python:3 RUN apt-get update &amp;amp;&amp;amp; apt-get install -y vim RUN python -m pip install pandas After creating the Dockerfile, the next step is to build and push the image to Artifact Registry.</description></item><item><title>Gorutines - deep dive into Go:s concurrency features</title><link>https://www.njordy.com/2023/08/15/gorutines/</link><pubDate>Tue, 15 Aug 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/08/15/gorutines/</guid><description>Gorutine high level Go:s native concurrency features is usually one of the first things developers bring up when describing the advantages of using go. However due to its eas of use it is also wildly miss used.
‚ÄúYou can have a second computer once you‚Äôve shown you know how to use the first one.‚Äù ‚ÄìPaul Barham
We will not deep dive in to when to use and not to use Gorutines specifically even though we touch upon some hints.</description></item><item><title>Rust: Scoped threads - easier multithreading</title><link>https://www.njordy.com/2023/07/30/rust-scoped-threads/</link><pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/07/30/rust-scoped-threads/</guid><description>The current development in CPU design is going towards large amount of cores rather than faster cores and thus writing parallel code becomes more important in order to utilize the full potential (Concurrency is not Parallelism). In this blog post we will dive into scoped threads, what it is and what is the difference between threads in rust in general. First of all only use threads if you need the speed up, introducing threads to a program adds complexity which both makes the program harder to maintain but if not done correct also slower to run(due to communications between threads and scheduling).</description></item><item><title>Rust Foreign data wrappers for postgres</title><link>https://www.njordy.com/2023/06/12/rust-foreign-data-wrappers-postgres-rust/</link><pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/06/12/rust-foreign-data-wrappers-postgres-rust/</guid><description>Background In this blog post we will try to implement a foreign data wrappers for postgres in rust. We will build on top of pgx in order to not have to build everything from the ground. But first of what is a foreign data wrapper? From the postgres docs:
A foreign data wrapper is a library that can communicate with an external data source, hiding the details of connecting to the data source and obtaining data from it.</description></item><item><title>DBOS: A Database-Oriented Operating System</title><link>https://www.njordy.com/2023/05/19/operating_system_databases/</link><pubDate>Fri, 19 May 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/05/19/operating_system_databases/</guid><description>A group of researches are proposing a radical change of the future operating system. Replacing the fundamental idea from Unix that everything is a file and instead relying on concepts from the database world a operating system that supports large scale distributed applications in the cloud can be built.
| Everything is a file
The core principles suggested to achieve this is:
Store all application in tables in a distributed database Store all OS state in tables in a distributed database.</description></item><item><title>Bloom filters</title><link>https://www.njordy.com/2023/05/17/bloom_filter/</link><pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/05/17/bloom_filter/</guid><description>Bloom filters is a probabilistic data structure, which is space efficient. Bloom filters can be used to quickly check if a value don&amp;rsquo;t exists or might exists in a set, false positives are possible(with a low likelihood) but false negatives are not possible. The time to check if an element exsist or add an element is also constant O(k), where k is the number of hash functions(we will covert this later).</description></item><item><title>Setting up a Basic dbt Development Container for BigQuery in GCP</title><link>https://www.njordy.com/2023/04/29/setting-up-basic-dbt-dev-container/</link><pubDate>Sat, 29 Apr 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/04/29/setting-up-basic-dbt-dev-container/</guid><description>In this post, you will learn how to set up a basic dbt project in Google Cloud Platform (GCP) and share a development container to kickstart your project. While there are numerous blog posts out there about dbt and BigQuery, none of them share how to set it up in a development container without using any of the dbt-cloud services (at least to my knowledge).
Setting upp your enviorment Set up a gcp project/ or take one you allready have</description></item><item><title>Go memory arenas for apache arrow, Part 2</title><link>https://www.njordy.com/2023/04/14/apache-arrow-memory-arena-go-part-2/</link><pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/04/14/apache-arrow-memory-arena-go-part-2/</guid><description>This blog post will continue to try to dive down in to apache arrow and specifically the Go memory allocation for Apache arrow. This is a follow up to Go memory arenas for apache arrow, Part 1.
First of all why do we want to manage memory manually instead of using the GC? One of arrows key features is it support to share memory with out copy between programs however for a GC collected language this will not work that great.</description></item><item><title>Push based query engine</title><link>https://www.njordy.com/2023/05/17push_based_query_engine/</link><pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/05/17push_based_query_engine/</guid><description>In this blog post we will dive down in to the difference between push based vs pull based query engines. As simple as is sounds push based is based upon that data is pushed from the sink through the different operators, this is used by snowflake and argued to be superior for OLAP which we will dive deeper into. Pull based have been around for a longer time and is based upon that data is pulled from the sink up through the operators, this is also known as theVolcano Iterator Model</description></item><item><title>Go memory arenas for apache arrow, Part 1</title><link>https://www.njordy.com/2023/03/23/apache-arrow-memory-arena-go/</link><pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/03/23/apache-arrow-memory-arena-go/</guid><description>This blog post will try to dive down in to apache arrow and specifically the Go memory allocation for Apache arrow. Apache arrow state that they allow for the following types of memory allocations:
Go default allocations(standard go GC collected memory) CGo allocator(memory allocated through CG0) Checked Memory Allocator Will deep dive in to these once in a follow up blog post. Today the goal is to extend with a new memory allocator, mostly becuase I read up on go memory arena which where introduced in to 1.</description></item><item><title>How to faster apply for a new job</title><link>https://www.njordy.com/2023/03/23/chatgpt3-cover-letter-generator/</link><pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/03/23/chatgpt3-cover-letter-generator/</guid><description>Applying for jobs are fun, but it is not fun to write cover letters at least not compare to hacking on some new open source tool or testing out something new. Therefore the goal is to reduce the time to apply for a job thus the goal today is to generate cover letters. For this we will need the following:
CV, I will copy paste mine from linkedin(minimal effort and I spend way to much time on linkedin so it is up to date) Job add(will take one from linkedin that sounds interesting) Get the CV.</description></item><item><title>Awesome go resources</title><link>https://www.njordy.com/2023/03/23/awesome-go/</link><pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/03/23/awesome-go/</guid><description>This blog post is a collection of awesome go resource and will be continuously update. The goal is that each resource should be describe shortly as well.
Blogs:
Generics can make your Go code slower Effective go Go memory model inline Videos:
Obscure Go Optimisations A Guide to the Go Garbage Collector</description></item><item><title>Storing Kubeflow Pipeline Templates in GCP Artifact Registry</title><link>https://www.njordy.com/2023/03/23/storing-kubeflow-pipeline-templates-gcp-artifact-registry/</link><pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/03/23/storing-kubeflow-pipeline-templates-gcp-artifact-registry/</guid><description>In this blog post, we will discuss how to store Kubeflow Pipeline templates in GCP Artifact Registry, enabling reusability and version control for your pipelines. Using Artifact Registry over Cloud Storage simplifies version control and allows for easier collaboration between single or multiple users.
The Kubeflow Pipelines SDK registry client is a new client interface that you can use with a compatible registry server (ensure you are using the correct KFP version), such as Artifact Registry, for version control of your Kubeflow Pipelines (KFP) templates.</description></item><item><title>Running a kubeflow pipeline on google vertex</title><link>https://www.njordy.com/2023/03/21/kubeflow-pipelines/</link><pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/03/21/kubeflow-pipelines/</guid><description>This blog post will go over how to build and run your very first kubeflow pipeline (kfp). In short, Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.
There are a lot of possibilities to run the pipelines, but in this series, we will use gcp vertex pipelines. Vertex will be the runner, but the pipelines will follow the kubeflow conventions meaning you can run them on whatever platform at hand or host kubeflow on your own Kubernetes cluster.</description></item><item><title>Configuring Your Local Dev Container with GCP Default Credentials</title><link>https://www.njordy.com/2023/03/20/vs-code-dev-container-gcp-credentials/</link><pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/03/20/vs-code-dev-container-gcp-credentials/</guid><description>This blog post provides a step-by-step guide for setting up your VS Code dev container to work with Google Cloud Platform (GCP) services and APIs by configuring default GCP credentials. By authenticating your application with your GCP credentials, you can access the necessary resources without requiring additional authentication steps, saving time and streamlining your development workflow.
Configuring Default GCP Credentials To use default credentials with GCP, you can follow the steps below:</description></item><item><title>Supercharge Your Development Workflow with VS Code Dev Containers</title><link>https://www.njordy.com/2023/03/18/dev-containers/</link><pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/03/18/dev-containers/</guid><description>VS docker Dev Container Are you tired of dealing with messy, inconsistent development environments that slow down your workflow? Look no further than VS Code dev containers, a powerful feature in Visual Studio Code that can streamline your development process and improve consistency across different environments.
In this post, we&amp;rsquo;ll dive into what dev containers are and how you can use them to supercharge your development workflow. We&amp;rsquo;ll also walk through how to set up a dev container for a Python environment and you will get an understsnding how easy it is to get started.</description></item><item><title>Postgress extention using ChatGPT3</title><link>https://www.njordy.com/2023/03/11/postgress_extention_using_chatgpt3/</link><pubDate>Sat, 11 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/03/11/postgress_extention_using_chatgpt3/</guid><description>This blog post will dive into building postgres extensions using pgx in rust. In order to do something existing we will ride the hype curve and integrate ChatGPT3 into postgres.
TLDR: repo
Setup First step is to set up pgx:
$ cargo install --locked cargo-pgx $ cargo pgx init We will then create a new create using:
$ cargo pgx new my_extension $ cd my_extension Which should give you something like:</description></item><item><title>Go memory arena</title><link>https://www.njordy.com/2023/03/01/go_memory_arena/</link><pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/03/01/go_memory_arena/</guid><description>As part of the go 1.20 release memory areas where introduced to the standard lib but not mentioned in the release notes but is still being discussed as of 2023-03-02 here. Memory arenas allow users to allocate memory and are described by the docs as:
The arena package provides the ability to allocate memory for a collection of Go values and free that space manually all at once, safely. The purpose of this functionality is to improve efficiency: manually freeing memory before a garbage collection delays that cycle.</description></item><item><title>Triton shared memory and pinned memory</title><link>https://www.njordy.com/2023/02/25/triton_shared_memory/</link><pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/02/25/triton_shared_memory/</guid><description>This blog post will go in to depth how to use shared memory together with nvidia triton and pinned memory for model serving. This will continue to build further on the other blog posts related to triton. First we will focuse on shared memory and then move over to also look in to pinned memory and why it matters.
Shared memory In the triton examples(python) shared memory is often abbreviated as shm.</description></item><item><title>Duckdb with hugo</title><link>https://www.njordy.com/2023/02/06/hugo-duckdb/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/02/06/hugo-duckdb/</guid><description>Current hack to just get the stuff working.</description></item><item><title>Go compiler optimizations</title><link>https://www.njordy.com/2023/02/08/Profile-guided_inlining_optimization/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/02/08/Profile-guided_inlining_optimization/</guid><description>This is based upon the new feature released in go v1.20 where the compiler can optimize using a pprof file.
In order to run the pprof we will use flags:
flag.Parse() if *cpuprofile != &amp;#34;&amp;#34; { f, err := os.Create(*cpuprofile) if err != nil { log.Fatal(err) } pprof.StartCPUProfile(f) defer pprof.StopCPUProfile() } more info can be found here.
In order to run the profiling use the following command:
go run main.go -cpuprofile=prof.</description></item><item><title>Shinylive app with hugo</title><link>https://www.njordy.com/2023/02/06/hugo-with-python-rshiny/</link><pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/02/06/hugo-with-python-rshiny/</guid><description>Shinylive app with hugo This blog post is based upon RamiKrispin/shinylive where we will take a look in to using Shiny with python and leveraging WebAssembly to let it run in the browser with out a backend. This allows for interactive static webpages.
In order to add the a shiny app it needs to be deployed, in this case that is handled through github pages and lives within https://github.com/NikeNano/shinylive. The second step is to add the iframe:</description></item><item><title>Trition with post and pre processing</title><link>https://www.njordy.com/2023/02/01/Trition_with_post_and_pre_processing/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/02/01/Trition_with_post_and_pre_processing/</guid><description>Trition with post and pre processing. This is based upon this repo
In this blog post we will dig down in to how a Machine Learning(ML) model can be combined with pre and post processing steps using Nvidia triton. By combining the pre- and post processing the user can make a single call using GPRC or http. It should be noted that we in reality will not merge these processing steps in any way but link the calls together using Tritons ensemble functionality Triton support multiple different backends(processing functionality) and in this case we will use the tensorRT backend for the model serving and the python backend to add the pre and post processing business logic.</description></item><item><title>Trition with post and pre processing</title><link>https://www.njordy.com/2023/02/01/Trition_with_post_and_pre_processing/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2023/02/01/Trition_with_post_and_pre_processing/</guid><description>Prompt engineering With the rise of GPT-3 and Stable diffusion the concpet of promt engineering has gain more and more traction. According to wikipedia the task can be described as
Prompt engineering typically works by converting one or more tasks to a prompt-based dataset and training a language model with what has been called &amp;ldquo;prompt-based learning&amp;rdquo; or just &amp;ldquo;prompt learning&amp;rdquo;
Wikipedia Prompts are inputs for models that expect text as input however the output can very most famously images or text.</description></item><item><title>Welcome to the Blog</title><link>https://www.njordy.com/2022/07/17/hello-world/</link><pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate><guid>https://www.njordy.com/2022/07/17/hello-world/</guid><description> ‚ÄúYeah It&amp;rsquo;s on. ‚Äù
Hello World!</description></item><item><title/><link>https://www.njordy.com/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.njordy.com/about/</guid><description>Niklas Niklas is a Software engineer(previously data scientist) working with battery manufacturing üîã . He loves learning new things and currently work on traceability for electrode manufacturing. Work is GO, python and SQL while night hacking currently is rust.
Some open source project Niklas has contributed to:
kubeflow pipelines argo workflows nvidia triton Johan Johan is a Machine Learning engineer working on forecasting and life time value modeling üîé .</description></item><item><title/><link>https://www.njordy.com/posts/2023-02-07-poor_mans_datalake/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.njordy.com/posts/2023-02-07-poor_mans_datalake/</guid><description>Poor mans datalake layout: post title: &amp;ldquo;Poor mans datalake&amp;rdquo; subtitle: &amp;ldquo;DuckDb&amp;rdquo; date: 2023-02-01 author: &amp;ldquo;Niklas Hansson&amp;rdquo; URL: &amp;ldquo;/2023/02/01/Trition_with_post_and_pre_processing/&amp;rdquo; This post is a deep dive playing with DuckDB doing a twist on Build a poor man‚Äôs data lake from scratch with DuckDB where we will do the following changes:
Use Minio instead of S3 DBT instead of dagster. We will host it on Kubernetes and set it up so it all run locally.</description></item></channel></rss>