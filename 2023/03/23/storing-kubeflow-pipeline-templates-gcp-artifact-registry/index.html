<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Storing Kubeflow Pipeline Templates in GCP Artifact Registry - Njord tech blog</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="In this blog post, we will discuss how to store Kubeflow Pipeline templates in GCP Artifact Registry, enabling reusability and version control for your pipelines. Using Artifact Registry over Cloud Storage simplifies version control and allows for easier collaboration between single or multiple users.
The Kubeflow Pipelines SDK registry client is a new client interface that you can use with a compatible registry server (ensure you are using the correct KFP version), such as Artifact Registry, for version control of your Kubeflow Pipelines (KFP) templates."><meta property="og:image" content><meta property="og:title" content="Storing Kubeflow Pipeline Templates in GCP Artifact Registry"><meta property="og:description" content="In this blog post, we will discuss how to store Kubeflow Pipeline templates in GCP Artifact Registry, enabling reusability and version control for your pipelines. Using Artifact Registry over Cloud Storage simplifies version control and allows for easier collaboration between single or multiple users.
The Kubeflow Pipelines SDK registry client is a new client interface that you can use with a compatible registry server (ensure you are using the correct KFP version), such as Artifact Registry, for version control of your Kubeflow Pipelines (KFP) templates."><meta property="og:type" content="article"><meta property="og:url" content="https://www.njordy.com/2023/03/23/storing-kubeflow-pipeline-templates-gcp-artifact-registry/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-23T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-23T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Storing Kubeflow Pipeline Templates in GCP Artifact Registry"><meta name=twitter:description content="In this blog post, we will discuss how to store Kubeflow Pipeline templates in GCP Artifact Registry, enabling reusability and version control for your pipelines. Using Artifact Registry over Cloud Storage simplifies version control and allows for easier collaboration between single or multiple users.
The Kubeflow Pipelines SDK registry client is a new client interface that you can use with a compatible registry server (ensure you are using the correct KFP version), such as Artifact Registry, for version control of your Kubeflow Pipelines (KFP) templates."><link href=https://www.njordy.com/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://www.njordy.com/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css></head><body><div class=content><header><div class=main><a href=https://www.njordy.com/>Njord tech blog</a></div><nav><a href=../../../../>Home</a>
<a href=../../../../posts>All posts</a>
<a href=../../../../about>About</a></nav></header><main><article><div class=title><h1 class=title>Storing Kubeflow Pipeline Templates in GCP Artifact Registry</h1><div class=meta>Posted on Mar 23, 2023</div></div><section class=body><p>In this blog post, we will discuss how to store Kubeflow Pipeline templates in GCP Artifact Registry, enabling reusability and version control for your pipelines. Using Artifact Registry over Cloud Storage simplifies version control and allows for easier collaboration between single or multiple users.</p><p><a href=https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template#use-the-template-in-kfp-client>The Kubeflow Pipelines SDK registry client is a new client interface that you can use with a compatible registry server (ensure you are using the correct KFP version), such as Artifact Registry, for version control of your Kubeflow Pipelines (KFP) templates. For more information, see &ldquo;Use the template in a Kubeflow Pipelines SDK registry client.</a></p><p>By using GCP Artifact Registry to store Kubeflow Pipeline templates, you can take advantage of its features, such as versioning, granular access control, and sharing of templates among team members. This allows for better management and organization of your machine learning workflows and MLOps processes.</p><p>Stay tuned for more blog posts in this series, where we will dive deeper into machine learning concepts and MLOps using Kubeflow Pipelines on GCP Vertex AI.</p><h2 id=setting-upp-your-enviorment>Setting upp your enviorment</h2><p>You can either use the dev container or follow the instructions <a href=https://njordy.com/2023/03/21/kubeflow-pipelines/>here</a>.</p><p>IMPORTETN! This turorial is built on using a higher version of KFP so we need to upgrade it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>    pip install kfp<span style=color:#f92672>==</span>2.0.0b13
</span></span></code></pre></div><p>You might have to update even that with the command below, when writting this post it is in beta.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>    kfp install kfp --pre
</span></span></code></pre></div><h2 id=creating-a-simple-pipeline-and-push-it-to-artifact-registry>Creating a simple pipeline and push it to artifact registry</h2><p>The documentation from gcp can be found <a href=https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template#vertex-ai-sdk-for-python_1>here</a></p><ol><li><p>Enable artifact registry in gcp.
<img src=../../../../img/enable_artifact_registry.png alt="artifact registry"></p></li><li><p>create an artifact registry
<img src=../../../../img/artifact_registry_kubeflow.png alt="Enable vertex"></p></li><li><p>We are now ready to start coding, first we create an env</p></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-env data-lang=env><span style=display:flex><span>    gcp_project<span style=color:#f92672>=</span>example-project
</span></span><span style=display:flex><span>    gcp_service_account<span style=color:#f92672>=</span>example-service-account@example-service-account.iam.gserviceaccount.com
</span></span><span style=display:flex><span>    bucket<span style=color:#f92672>=</span>gs://example-bucket
</span></span><span style=display:flex><span>    kubeflow_pipelines_artifact_registyr<span style=color:#f92672>=</span>test-test
</span></span></code></pre></div><ol start=4><li>Time to start coding in python</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> kfp.registry <span style=color:#f92672>import</span> RegistryClient
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> dotenv <span style=color:#f92672>import</span> load_dotenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    load_dotenv()
</span></span><span style=display:flex><span>    bucket <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;bucket&#34;</span>)
</span></span><span style=display:flex><span>    gcp_project <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;gcp_project&#34;</span>)
</span></span><span style=display:flex><span>    gcp_service_account <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;gcp_service_account&#34;</span>)
</span></span><span style=display:flex><span>    kubeflow_pipelines_artifact_registyr <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#39;kubeflow_pipelines_artifact_registyr&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    client <span style=color:#f92672>=</span> RegistryClient(host<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;https://europe-west1-kfp.pkg.dev/</span><span style=color:#e6db74>{</span>gcp_project<span style=color:#e6db74>}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>{</span>kubeflow_pipelines_artifact_registyr<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>This code snippet demonstrates how to set up a connection to a Kubeflow Pipelines Artifact Registry using the RegistryClient from the kfp.registry module. It first loads the necessary environment variables from a .env file using the dotenv package, retrieves the required environment variables, and then establishes a connection to the Artifact Registry using the RegistryClient.</p><ol start=5><li>List all artifacts.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    client<span style=color:#f92672>.</span>list_packages()
</span></span></code></pre></div><ol start=6><li>Let&rsquo;s create a simple kubeflow template.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#f92672>import</span> kfp.dsl <span style=color:#66d9ef>as</span> dsl
</span></span><span style=display:flex><span>    <span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> dotenv <span style=color:#f92672>import</span> load_dotenv
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> kfp.v2.dsl <span style=color:#f92672>import</span> component
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> kfp.v2 <span style=color:#f92672>import</span> compiler
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> google.cloud <span style=color:#f92672>import</span> aiplatform <span style=color:#66d9ef>as</span> aip
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Define the model training function</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_model</span>(input: float) <span style=color:#f92672>-&gt;</span> float:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2.0</span> <span style=color:#f92672>+</span> input
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Define the data ingestion function</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ingetst_data</span>(input: float) <span style=color:#f92672>-&gt;</span> float:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create components for the ingestion and training functions</span>
</span></span><span style=display:flex><span>    ingest_data_component <span style=color:#f92672>=</span> component(ingetst_data)
</span></span><span style=display:flex><span>    train_component <span style=color:#f92672>=</span> component(train_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Define the pipeline using the Kubeflow Pipelines SDK</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@dsl.pipeline</span>(
</span></span><span style=display:flex><span>        name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ltv-train&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_pipeline</span>():
</span></span><span style=display:flex><span>        <span style=color:#75715e># Instantiate the ingest_data_component and store its output</span>
</span></span><span style=display:flex><span>        ingest_data <span style=color:#f92672>=</span> ingest_data_component(input<span style=color:#f92672>=</span><span style=color:#ae81ff>3.0</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Instantiate the train_component, passing the output from the ingest_data_component</span>
</span></span><span style=display:flex><span>        train_model <span style=color:#f92672>=</span> train_component(input<span style=color:#f92672>=</span>ingest_data<span style=color:#f92672>.</span>output)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Disable caching for the train_model component to ensure it runs every time</span>
</span></span><span style=display:flex><span>        train_model<span style=color:#f92672>.</span>set_caching_options(<span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Compile the pipeline to generate a YAML file for execution</span>
</span></span><span style=display:flex><span>    compiler<span style=color:#f92672>.</span>Compiler()<span style=color:#f92672>.</span>compile(pipeline_func<span style=color:#f92672>=</span>add_pipeline, package_path<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;local_run.yaml&#34;</span>)
</span></span></code></pre></div><p>This code snipet demonstrates how to create a simple Kubeflow pipeline using the Kubeflow Pipelines SDK. It defines two functions, one for data ingestion and another for model training. Components are created for both functions using the component function from the kfp.v2.dsl module. The pipeline is then defined using the @dsl.pipeline decorator, which instantiates the components and sets the desired caching options. Finally, the pipeline is compiled into a YAML file for execution.</p><ol start=7><li>Let&rsquo;s push the templat to artifact registry.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#75715e># Upload the pipeline to the Kubeflow Pipelines registry</span>
</span></span><span style=display:flex><span>    templateName, versionName <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>upload_pipeline(
</span></span><span style=display:flex><span>        <span style=color:#75715e># Provide the compiled pipeline YAML file</span>
</span></span><span style=display:flex><span>        file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;local_run.yaml&#34;</span>,
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Assign tags to the pipeline for easier identification and versioning</span>
</span></span><span style=display:flex><span>        tags<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;v1&#34;</span>, <span style=color:#e6db74>&#34;latest&#34;</span>],
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Add a description to the pipeline using extra_headers</span>
</span></span><span style=display:flex><span>        extra_headers<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;description&#34;</span>: <span style=color:#e6db74>&#34;This is an example pipeline template.&#34;</span>}
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>This code snippet uploads the compiled pipeline YAML file (local_run.yaml) to the Kubeflow Pipelines registry. The upload_pipeline function is used to submit the pipeline with specified tags and a description. The tags help with versioning and identification of the pipeline, while the description provides additional context about the pipeline&rsquo;s purpose.</p><p>You should now have something like this but with one version in gcp.
<img src=../../../../img/artifact_registry_kubeflow_versions.png alt="Alt text">. You can find the code here in <a href=https://github.com/Njorda/kubeflow-pipelines/blob/main/notebooks/vertex_artifact_registry_kubeflow_pipelines.ipynb>github</a></p></section><div class=post-tags></div></article></main><footer><div style=display:flex></div><div class=footer-info>2023 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer></div></body></html>