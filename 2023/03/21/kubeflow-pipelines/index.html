<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Running a kubeflow pipeline on google vertex - Njord tech blog</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This blog post will go over how to build and run your very first kubeflow pipeline (kfp). In short, Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.
There are a lot of possibilities to run the pipelines, but in this series, we will use gcp vertex pipelines. Vertex will be the runner, but the pipelines will follow the kubeflow conventions meaning you can run them on whatever platform at hand or host kubeflow on your own Kubernetes cluster."><meta property="og:image" content><meta property="og:title" content="Running a kubeflow pipeline on google vertex"><meta property="og:description" content="This blog post will go over how to build and run your very first kubeflow pipeline (kfp). In short, Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.
There are a lot of possibilities to run the pipelines, but in this series, we will use gcp vertex pipelines. Vertex will be the runner, but the pipelines will follow the kubeflow conventions meaning you can run them on whatever platform at hand or host kubeflow on your own Kubernetes cluster."><meta property="og:type" content="article"><meta property="og:url" content="https://njorda.github.io/2023/03/21/kubeflow-pipelines/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-21T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-21T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Running a kubeflow pipeline on google vertex"><meta name=twitter:description content="This blog post will go over how to build and run your very first kubeflow pipeline (kfp). In short, Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.
There are a lot of possibilities to run the pipelines, but in this series, we will use gcp vertex pipelines. Vertex will be the runner, but the pipelines will follow the kubeflow conventions meaning you can run them on whatever platform at hand or host kubeflow on your own Kubernetes cluster."><link href=https://njorda.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://njorda.github.io/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css></head><body><div class=content><header><div class=main><a href=https://njorda.github.io/>Njord tech blog</a></div><nav><a href=../../../../>Home</a>
<a href=../../../../posts>All posts</a>
<a href=../../../../about>About</a></nav></header><main><article><div class=title><h1 class=title>Running a kubeflow pipeline on google vertex</h1><div class=meta>Posted on Mar 21, 2023</div></div><section class=body><p>This blog post will go over how to build and run your very first kubeflow pipeline (kfp). In short, <a href=https://www.kubeflow.org/docs/components/pipelines/v1/introduction/>Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.</a></p><p>There are a lot of possibilities to run the pipelines, but in this series, we will use <a href=https://cloud.google.com/vertex-ai/docs/pipelines/introduction>gcp vertex pipelines</a>. Vertex will be the runner, but the pipelines will follow the kubeflow conventions meaning you can run them on whatever platform at hand or host kubeflow on your own Kubernetes cluster.</p><p>This is the first in a series of kubeflow posts and will showcase the basis; the focus is not on ML but rather how to use it. The following ones will focus more on machine learning concepts and MLOps.</p><h2 id=creating-a-basic-kubeflow-pipeline>Creating a basic kubeflow pipeline</h2><p>First, we need to install some packages and set up Python. If you want, you can clone the repo, start the dev-container, and you can skip this step. We highly recommend this instead of spending time on your environment, as we will only cover it briefly here.</p><p>First, you need to install <code>python:3.10</code>, which we use in this tutorial.</p><p>Install the following packages and versions</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>    pandas<span style=color:#f92672>==</span>1.5.3
</span></span><span style=display:flex><span>    kfp<span style=color:#f92672>==</span>2.0.0b13
</span></span><span style=display:flex><span>    ipykernel
</span></span><span style=display:flex><span>    google-cloud-aiplatform<span style=color:#f92672>==</span>1.23.0
</span></span><span style=display:flex><span>    python-dotenv<span style=color:#f92672>==</span>1.0.0
</span></span><span style=display:flex><span>    black 
</span></span><span style=display:flex><span>    black<span style=color:#f92672>[</span>jupyter<span style=color:#f92672>]</span>
</span></span></code></pre></div><p>Most of the code will happen in a Jupyter notebook, so you also need to install Jupyter.</p><h2 id=setting-up-you-gcp-enviroment>Setting up you gcp enviroment</h2><p>The code below can be found <a href=https://github.com/Njorda/kubeflow-pipelines>here</a> and the the notebook to use is first_kubeflow_pipelines.ipnyb</p><ol><li><p>Create a new gcp project and add a billing account (There will be close to 0 costs here, but you need to enable it to use vertex)</p></li><li><p>Enable the following APIs; if you want to read more, you can find the information <a href=https://cloud.google.com/vertex-ai/docs/pipelines/configure-project>here</a></p></li></ol><ul><li><p>Compute Engine API</p></li><li><p>Vertex AI API <img src=../../../../img/enable_vertex.png alt="Enable vertex"></p></li><li><p>Cloud Storage</p></li></ul><ol start=3><li><p>Create a service account with vertex AI access; an ugly way that works is to create an account that has owner access.</p></li><li><p>Ok, now it is time to start coding; the first step is to create an .env file and add the service account, the bucket name, and the gcp project</p></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-env data-lang=env><span style=display:flex><span>    gcp_project<span style=color:#f92672>=</span>example-project
</span></span><span style=display:flex><span>    gcp_service_account<span style=color:#f92672>=</span>example-service-account@example-service-account.iam.gserviceaccount.com
</span></span><span style=display:flex><span>    bucket<span style=color:#f92672>=</span>gs://example-bucket
</span></span></code></pre></div><ol start=5><li>The first step is to import the needed functions and see if we have access to list all previous run pipelines in vertex.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>load_ext autoreload
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>autoreload <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> kfp.dsl <span style=color:#66d9ef>as</span> dsl
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dotenv <span style=color:#f92672>import</span> load_dotenv
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> kfp.v2.dsl <span style=color:#f92672>import</span> Output, component, InputPath, HTML, Input, Dataset
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> kfp.v2 <span style=color:#f92672>import</span> compiler
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> google.cloud <span style=color:#f92672>import</span> aiplatform <span style=color:#66d9ef>as</span> aip
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#loading enviroment variables from the .env file</span>
</span></span><span style=display:flex><span>load_dotenv()
</span></span><span style=display:flex><span>bucket <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;bucket&#34;</span>)
</span></span><span style=display:flex><span>gcp_project <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;gcp_project&#34;</span>)
</span></span><span style=display:flex><span>gcp_service_account <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;gcp_service_account&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>aip<span style=color:#f92672>.</span>init(
</span></span><span style=display:flex><span>    project<span style=color:#f92672>=</span>gcp_project,
</span></span><span style=display:flex><span>    location<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;europe-west1&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>all_piplines <span style=color:#f92672>=</span> aip<span style=color:#f92672>.</span>PipelineJob<span style=color:#f92672>.</span>list()
</span></span><span style=display:flex><span>all_piplines
</span></span></code></pre></div><p>This should list all previous kfp pipelines, if your following this blog post it will probably be an empty list.</p><ol start=6><li>Lets create an super simple pipeline.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_model</span>(input: float) <span style=color:#f92672>-&gt;</span> float:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2.0</span> <span style=color:#f92672>+</span> input
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ingetst_data</span>() <span style=color:#f92672>-&gt;</span> float:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create components for the ingestion and training functions</span>
</span></span><span style=display:flex><span>    ingest_data_component <span style=color:#f92672>=</span> component(ingetst_data)
</span></span><span style=display:flex><span>    train_component <span style=color:#f92672>=</span> component(train_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Define the pipeline using the Kubeflow Pipelines SDK</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@dsl.pipeline</span>(
</span></span><span style=display:flex><span>        name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ltv-train&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_pipeline</span>():
</span></span><span style=display:flex><span>        <span style=color:#75715e># Instantiate the ingest_data_component and store its output</span>
</span></span><span style=display:flex><span>        ingest_data <span style=color:#f92672>=</span> ingest_data_component()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Instantiate the train_component, passing the output from the ingest_data_component</span>
</span></span><span style=display:flex><span>        train_model <span style=color:#f92672>=</span> train_component(input<span style=color:#f92672>=</span>ingest_data<span style=color:#f92672>.</span>output)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Disable caching for the train_model component to ensure it runs every time</span>
</span></span><span style=display:flex><span>        train_model<span style=color:#f92672>.</span>set_caching_options(<span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Compile the pipeline to generate a JSON file for execution</span>
</span></span><span style=display:flex><span>    compiler<span style=color:#f92672>.</span>Compiler()<span style=color:#f92672>.</span>compile(pipeline_func<span style=color:#f92672>=</span>add_pipeline, package_path<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;local_run.yaml&#34;</span>)
</span></span></code></pre></div><p>This code defines a simple pipeline using the Kubeflow Pipelines SDK. The pipeline consists of two components: a data ingestion component (ingetst_data) and a model training component (train_model). The ingetst_data component returns a constant value of 2.0, while the train_model component adds 2.0 to the input value. Finally, the pipeline is compiled and saved as a JSON file (local_run.json) for local execution or deployment.</p><ol start=7><li>To execute the pipeline on vertex you run the following code</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    job <span style=color:#f92672>=</span> aip<span style=color:#f92672>.</span>PipelineJob(
</span></span><span style=display:flex><span>        display_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;First kubeflow pipeline&#34;</span>,
</span></span><span style=display:flex><span>        template_path<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;local_run.yaml&#34;</span>,
</span></span><span style=display:flex><span>        pipeline_root<span style=color:#f92672>=</span>bucket,
</span></span><span style=display:flex><span>        location<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;europe-west1&#34;</span>,
</span></span><span style=display:flex><span>        project<span style=color:#f92672>=</span>gcp_project,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    job<span style=color:#f92672>.</span>submit(
</span></span><span style=display:flex><span>        service_account<span style=color:#f92672>=</span>gcp_service_account
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>You should now get an output with a link the the running pipeline if you follow it you should see somthing like this. <img src=../../../../img/vertex_pipeline.png alt="vertex output"></p></section><div class=post-tags></div></article></main><footer><div style=display:flex></div><div class=footer-info>2023 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer></div></body></html>