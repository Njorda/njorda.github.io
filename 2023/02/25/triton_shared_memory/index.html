<!doctype html><html><head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge"><title>Triton shared memory and pinned memory - Njord tech blog</title><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="This blog post will go in to depth how to use shared memory together with nvidia triton and pinned memory for model serving. This will continue to build further on the other blog posts related to triton. First we will focuse on shared memory and then move over to also look in to pinned memory and why it matters.
Shared memory In the triton examples(python) shared memory is often abbreviated as shm.">
<meta property="og:image" content>
<meta property="og:title" content="Triton shared memory and pinned memory">
<meta property="og:description" content="This blog post will go in to depth how to use shared memory together with nvidia triton and pinned memory for model serving. This will continue to build further on the other blog posts related to triton. First we will focuse on shared memory and then move over to also look in to pinned memory and why it matters.
Shared memory In the triton examples(python) shared memory is often abbreviated as shm.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://Njorda.github.io/2023/02/25/triton_shared_memory/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2023-02-25T00:00:00+00:00">
<meta property="article:modified_time" content="2023-02-25T00:00:00+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Triton shared memory and pinned memory">
<meta name=twitter:description content="This blog post will go in to depth how to use shared memory together with nvidia triton and pinned memory for model serving. This will continue to build further on the other blog posts related to triton. First we will focuse on shared memory and then move over to also look in to pinned memory and why it matters.
Shared memory In the triton examples(python) shared memory is often abbreviated as shm.">
<link href=https://Njorda.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet>
<link rel=stylesheet type=text/css media=screen href=https://Njorda.github.io/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css>
</head>
<body>
<div class=content><header>
<div class=main>
<a href=https://Njorda.github.io>Njord tech blog</a>
</div>
<nav>
<a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
</nav>
</header>
<main>
<article>
<div class=title>
<h1 class=title>Triton shared memory and pinned memory</h1>
<div class=meta>Posted on Feb 25, 2023</div>
</div>
<section class=body>
<p>This blog post will go in to depth how to use <a href=https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_shared_memory.md>shared memory</a> together with <a href=https://developer.nvidia.com/nvidia-triton-inference-server>nvidia triton</a> and <a href=https://github.com/triton-inference-server/server/blob/dccc3b43df120f0339ad9c8d262ddddbce7b0ba3/src/main.cc#L593>pinned memory</a> for model serving. This will continue to build further on the other blog posts related to triton. First we will focuse on shared memory and then move over to also look in to pinned memory and why it matters.</p>
<h1 id=shared-memory>Shared memory</h1>
<p>In the triton <a href=https://github.com/triton-inference-server/client/tree/main/src/python/examples>examples(python)</a> shared memory is often abbreviated as shm. But what is shared memory and why does it matter? The documentation describes the benefits simply as:</p>
<blockquote>
<p>The shared-memory extensions allow a client to communicate input and output tensors by system or CUDA shared memory. Using shared memory instead of sending the tensor data over the GRPC or REST interface can provide significant performance improvement for some use cases.<a href=https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_shared_memory.md>link</a></p>
</blockquote>
<p>Thus can be summarize as it allows us to send a reference to memory instead of the data around. A more in depth blog post for shared memory for cuda can be found <a href=https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/>here</a>.</p>
<p>In this cas wel will us a single example that can be run locally and we will focus on system shared memory and not GPU shared memory but the logic will be the same.</p>
<h2 id=analyze-performance-using-pref_analyzerhttpsgithubcomtriton-inference-serverclientblobmainsrccperf_analyzerreadmemdshared-memory>Analyze performance using <a href=https://github.com/triton-inference-server/client/blob/main/src/c++/perf_analyzer/README.md#shared-memory>pref_analyzer</a></h2>
<p>In order to see if this is actually worth doing <a href=https://github.com/triton-inference-server/client/blob/main/src/c++/perf_analyzer/README.md#shared-memory>pref_analyzer</a>(tool to analyze performance for triton) can be used.</p>
<p>If you have issue setting it up check <a href=https://github.com/triton-inference-server/server/issues/4479>this github issue</a>.</p>
<p>To start the docker container(since we dont like to install things and it is 2023 and docker is making life easier):</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>docker run -it --network host --shm-size<span style=color:#f92672>=</span>10gb --ipc host --ulimit memlock<span style=color:#f92672>=</span>-1 -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>:/workspace/src nvcr.io/nvidia/tritonserver:23.01-py3-sdk /bin/bash
</code></pre></div><p>Command to run:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>perf_analyzer -m YOU_MODEL_NAME_HERE -u localhost:8001 -i gRPC --input-data YOUR_INPUT_DATA.json input.json
</code></pre></div><h1 id=pinned-memory>Pinned memory</h1>
<p><a href=https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/>Nvidia blog post</a> about pinned memory and why it matters.</p>
</section>
<div class=post-tags>
</div>
</article>
</main>
<footer>
<div style=display:flex></div>
<div class=footer-info>
2023 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a>
</div>
</footer>
</div>
</body>
</html>