<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Profile nvidia triton model server - Njord tech blog</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="In order to profile the triton on the GPU we will use NVIDIA Nsight Systems. Installation instructions can be found here. Older version of Nsigh systems can be found here.The first step is to generate a nsys report. This can be done using the following command(you need to continue reading if you want to get all the traces and build NVIDIA triton image):
nsys profile --output /MY_OUTPUT_FOLDER/tmp.nsys-rep tritonserver --model-repository In this blog post we will assume that the GPU is connected to a remote machine that you can run docker or kubernetes on."><meta property="og:image" content><meta property="og:title" content="Profile nvidia triton model server"><meta property="og:description" content="In order to profile the triton on the GPU we will use NVIDIA Nsight Systems. Installation instructions can be found here. Older version of Nsigh systems can be found here.The first step is to generate a nsys report. This can be done using the following command(you need to continue reading if you want to get all the traces and build NVIDIA triton image):
nsys profile --output /MY_OUTPUT_FOLDER/tmp.nsys-rep tritonserver --model-repository In this blog post we will assume that the GPU is connected to a remote machine that you can run docker or kubernetes on."><meta property="og:type" content="article"><meta property="og:url" content="https://www.njordy.com/2024/04/07/nsys_profile/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-01T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Profile nvidia triton model server"><meta name=twitter:description content="In order to profile the triton on the GPU we will use NVIDIA Nsight Systems. Installation instructions can be found here. Older version of Nsigh systems can be found here.The first step is to generate a nsys report. This can be done using the following command(you need to continue reading if you want to get all the traces and build NVIDIA triton image):
nsys profile --output /MY_OUTPUT_FOLDER/tmp.nsys-rep tritonserver --model-repository In this blog post we will assume that the GPU is connected to a remote machine that you can run docker or kubernetes on."><link href=https://www.njordy.com/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://www.njordy.com/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css></head><body><div class=content><header><div class=main><a href=https://www.njordy.com/>Njord tech blog</a></div><nav><a href=../../../../>Home</a>
<a href=../../../../posts>All posts</a>
<a href=../../../../about>About</a></nav></header><main><article><div class=title><h1 class=title>Profile nvidia triton model server</h1><div class=meta>Posted on Mar 1, 2024</div></div><section class=body><p>In order to profile the triton on the GPU we will use <a href=https://developer.nvidia.com/nsight-systems>NVIDIA Nsight Systems</a>. Installation instructions can be found <a href=https://developer.nvidia.com/nsight-systems/get-started>here</a>. Older version of Nsigh systems can be found <a href="https://developer.nvidia.com/gameworksdownload#?dn=nvidia-nsight-graphics--2023-4">here</a>.The first step is to generate a nsys report. This can be done using the following command(you need to continue reading if you want to get all the traces and build NVIDIA triton image):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>nsys profile --output /MY_OUTPUT_FOLDER/tmp.nsys-rep tritonserver --model-repository
</span></span></code></pre></div><p>In this blog post we will assume that the GPU is connected to a remote machine that you can run docker or kubernetes on. You will NVIDIA Nsight Systems locally and load the saved profiling report from triton. This can be fetched from k8 with something like the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl --context YOUR_CONTEXT-n YOUR_NAMESPACE cp TRITON_POD:/MY_OUTPUT_FOLDER/tmp.nsys-rep out.nsys-rep
</span></span></code></pre></div><p>The command will sporadically output an output error but should still work.</p><p>The NVIDIA Triton docker images is built with support for <code>nsys</code> cli, however, they do not contain the cuda NVTX markers by default. If you want to use NVTX markers, you have to build Triton with <code>build.py</code>, using the “&ndash;enable-nvtx” flag. This will provide details around some phases of processing a request, such as queueing, running inference, and handling outputs. Thus we will now work over how to build the triton docker image with NVTX markers added. More info on debugging can be found <a href=https://github.com/triton-inference-server/server/blob/main/docs/user_guide/debugging_guide.md>here</a>.</p><p>Nest step is to check out branch of triton. We will build the <code>2.41.0</code> version specifically.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git checkout git@github.com:triton-inference-server/server.git
</span></span><span style=display:flex><span>git checkout r24.01
</span></span></code></pre></div><p>We stay on this version since some of the more recent once have had issues with ONNX which we will use for our example. After failing multiple times on the build I decided to jump on to a beefy machine(76 GM RAM and a lot of cores with 400 GB disc) on AWS with Ubuntu to make the build which turned out to be an amazing decision since it removed the errors and made the build fast and successful.</p><p>Time to build the actual image, there are a lot of options and ended up having to set <code>--enable-all</code> in order to get it to work, might be something more streamlined but this is how I did it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo ./build.py --enable-all
</span></span></code></pre></div><p>In this blog post we will dive deep in to the memory profiling specifically, <a href="https://www.youtube.com/watch?v=GCkdiHk6fUY">this video</a> is a really good introduction to memory Analysis with NVIDIA Nsight Compute, which can be installed from <a href=https://developer.nvidia.com/tools-overview/nsight-compute/get-started>here</a>. NVIDIA Nsight Compute vs NVIDIA Nsight systems can be found <a href=https://giahuy04.medium.com/introduction-to-nsight-systems-nsight-compute-642ff9578f9f>here</a>.</p><p>The next step is to run triton with nsys:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>nsys profile --output --force-overwrite<span style=color:#f92672>=</span>true /MY_OUTPUT_FOLDER/tmp.nsys-rep tritonserver --model-repository
</span></span></code></pre></div><p>In order to get the report you need to shut down the server &ldquo;ctrl + c&rdquo; just press once and wait for nsys to collect the traces you should get something like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>I0419 19:21:20.191207 <span style=color:#ae81ff>159401</span> grpc_server.cc:2519<span style=color:#f92672>]</span> Started GRPCInferenceService at 0.0.0.0:8001
</span></span><span style=display:flex><span>I0419 19:21:20.191732 <span style=color:#ae81ff>159401</span> http_server.cc:4623<span style=color:#f92672>]</span> Started HTTPService at 0.0.0.0:8000
</span></span><span style=display:flex><span>I0419 19:21:20.235119 <span style=color:#ae81ff>159401</span> http_server.cc:315<span style=color:#f92672>]</span> Started Metrics Service at 0.0.0.0:8002
</span></span><span style=display:flex><span>^CSignal <span style=color:#f92672>(</span>2<span style=color:#f92672>)</span> received.
</span></span><span style=display:flex><span>I0419 19:23:17.167877 <span style=color:#ae81ff>159401</span> server.cc:307<span style=color:#f92672>]</span> Waiting <span style=color:#66d9ef>for</span> in-flight requests to complete.
</span></span><span style=display:flex><span>I0419 19:23:17.167903 <span style=color:#ae81ff>159401</span> server.cc:323<span style=color:#f92672>]</span> Timeout 30: Found <span style=color:#ae81ff>0</span> model versions that have in-flight inferences
</span></span><span style=display:flex><span>I0419 19:23:17.168044 <span style=color:#ae81ff>159401</span> server.cc:338<span style=color:#f92672>]</span> All models are stopped, unloading models
</span></span><span style=display:flex><span>I0419 19:23:17.168061 <span style=color:#ae81ff>159401</span> server.cc:345<span style=color:#f92672>]</span> Timeout 30: Found <span style=color:#ae81ff>1</span> live models and <span style=color:#ae81ff>0</span> in-flight non-inference requests
</span></span><span style=display:flex><span>Generating <span style=color:#e6db74>&#39;/tmp/nsys-report-bacf.qdstrm&#39;</span>
</span></span><span style=display:flex><span>I0419 19:23:17.169166 <span style=color:#ae81ff>159401</span> onnxruntime.cc:2838<span style=color:#f92672>]</span> TRITONBACKEND_ModelInstanceFinalize: delete instance state
</span></span><span style=display:flex><span>I0419 19:23:17.379429 <span style=color:#ae81ff>159401</span> onnxruntime.cc:2762<span style=color:#f92672>]</span> TRITONBACKEND_ModelFinalize: delete model state
</span></span><span style=display:flex><span>I0419 19:23:17.477649 <span style=color:#ae81ff>159401</span> model_lifecycle.cc:612<span style=color:#f92672>]</span> successfully unloaded <span style=color:#e6db74>&#39;model&#39;</span> version <span style=color:#ae81ff>202404091557</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>1/1<span style=color:#f92672>]</span> <span style=color:#f92672>[=</span>16%                        <span style=color:#f92672>]</span> test.nsys-repI0419 19:23:18.168230 <span style=color:#ae81ff>159401</span> server.cc:345<span style=color:#f92672>]</span> Timeout 29: Found <span style=color:#ae81ff>0</span> live models and <span style=color:#ae81ff>0</span> in-flight non-inference requests
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>1/1<span style=color:#f92672>]</span> <span style=color:#f92672>[========================</span>100%<span style=color:#f92672>]</span> test.nsys-rep
</span></span><span style=display:flex><span>Generated:
</span></span><span style=display:flex><span>    /test.nsys-rep
</span></span></code></pre></div><p>If you see strange errors that involve:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>...
</span></span><span style=display:flex><span>magic number mismatch
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><ul><li>possible there is a msissmatch between the triton container nsys <code>nsys --version</code> and NVIDIA Nsight Systems.</li><li>It could also be as suppressed error and you fail to write <code>Failed to create '/opt/tritonserver/report1': Permission denied.</code> and needs to add a volume to which you have write access.</li></ul><p>Take the file down locally using kubectl cp, example below. DONT BELEVE IN THE ERROR MESSAGE IT IS A SCAM:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl --context CONTEXT -n NAMESPACE  cp POD_NAME:/models/out.nsys-rep out.nsys-rep
</span></span></code></pre></div><p>Now it is time to load up the report in Nsight.</p><p>Good luck and hope you manage to squeeze out some extra performance from the GPU!</p></section><div class=post-tags></div></article></main><footer><div style=display:flex></div><div class=footer-info>2024 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer></div></body></html>